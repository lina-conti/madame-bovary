{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZgKHfkBwh2LA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLyS0rZh9Ql"
      },
      "source": [
        "Goal\n",
        "==\n",
        "We are about to train a *sequence-to-sequence model* to predict a paragraph of Gustave Flaubert's *Madame Bovary* given the preceding paragraph.\n",
        "The model (at least in its first version) does not use words as units of text but characters.\n",
        "\n",
        "*   The encoder part, based on a bidirectional LSTM, reads an input paragraph and turns it into a set of tensors that serves as initial state for the decoder part.\n",
        "*   The decoder part is based on an (unidirectional) LSTM. The state of the LSTM is used to compute a probability distribution over the alphabet (including space and punctuation marks) and is updated each time a character is predicted by the LSTM reading this character's embedding.\n",
        "*   The goal is to get the best model. It is part of the job to define what this means. It is also part of the job to explain me how you get your best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytOCVOAXoO2s"
      },
      "source": [
        "This is an assignment.\n",
        "==\n",
        "\n",
        "*   Work in groups of two or three students.\n",
        "*   Due date: December 12th (Monday), 23:59\n",
        "*   Malus: -1 per day of delay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWD-p9yiECs"
      },
      "source": [
        "Loading PyTorch is important.\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An16FNHuhZI1"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarks:\n",
        "==\n",
        "*   Follow the instructions very carefully. Do not ignore any comment.\n",
        "*   Comment your code (including the role of all functions and the type of their arguments). A piece of code not appropriately commented can be considered incorrect (irrespectively of whether it works or not).\n",
        "*   Indicate the shape of each tensor that you define.\n",
        "*   Document all the changes that you make. Any work that is not properly explained can be ignored."
      ],
      "metadata": {
        "id": "b8IdgwKsx1P2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is there: \"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\"\n",
        "\n",
        "We have to pre-process it a little bit in order to remove everything that is not part of the text and to split the actual text into paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VXDpa1tiSfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c342caf-28fb-4786-f128-722a9722ffc5"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "#to train on french data change this variable to 'fr'\n",
        "USER_LG = 'fr'\n",
        "\n",
        "#to change between word and character tokenization, change this to 'word' or char'\n",
        "TOKENIZATION = 'word'\n",
        "\n",
        "tmp  =  urllib.request.urlretrieve('https://www.gutenberg.org/cache/epub/67138/pg67138.txt') if USER_LG == 'en' else\\\n",
        "        urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")\n",
        "filename = tmp[0]\n",
        "print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpr0exwptj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4hirpsiWX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b40451-2507-4431-9db9-b24dfa7069a8"
      },
      "source": [
        "# Prints the first 200 lines in the file with their line number.\n",
        "# This shows that we have a little bit of preprocessing to do in order to clean the data.\n",
        "with open(filename) as f:\n",
        "  for i in range(20):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ﻿The Project Gutenberg EBook of Madame Bovary, by Gustave Flaubert\n",
            "[1] \n",
            "[2] This eBook is for the use of anyone anywhere at no cost and with\n",
            "[3] almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "[4] re-use it under the terms of the Project Gutenberg License included\n",
            "[5] with this eBook or online at www.gutenberg.org\n",
            "[6] \n",
            "[7] \n",
            "[8] Title: Madame Bovary\n",
            "[9] \n",
            "[10] Author: Gustave Flaubert\n",
            "[11] \n",
            "[12] Release Date: November 26, 2004 [EBook #14155]\n",
            "[13] [Last updated: November 28, 2011]\n",
            "[14] \n",
            "[15] \n",
            "[16] Language: French\n",
            "[17] \n",
            "[18] \n",
            "[19] *** START OF THIS PROJECT GUTENBERG EBOOK MADAME BOVARY ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctl4Z9Gti-6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b941fd3c-ace7-42c9-8483-92731256b615"
      },
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "END_OF_BOOK = 'THE END' if USER_LG == 'en' else \\\n",
        "              'End of the Project Gutenberg EBook of Madame Bovary, by Gustave Flaubert'\n",
        "\n",
        "with open(filename) as f:\n",
        "  # We want to skip everything before the actual text of the novel.\n",
        "  # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "  # The following lines discard everything up to this second occurence (included).\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    # The two langauges have different preprocessing needs, but since they're in different langauge we don't need to worry about overlap\n",
        "    if(line == \"BOOK I\"): skip -= 2\n",
        "    if(line.startswith('PREMIÈRE PARTIE')): skip -=1 \n",
        "\n",
        "  paragraphs = [] # Note that each dialog line will be considered a separate paragraph.\n",
        "  paragraph_buffer = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"End of the Project Gutenberg EBook of Madame Bovary, by Gustave Flaubert\"): break # End of the actual text (fr).\n",
        "    if(line == END_OF_BOOK): break # End of the actual text (en).\n",
        "\n",
        "    if(line == \"\"): # We've reached the end of a paragraph.\n",
        "      if(len(paragraph_buffer) > 0):\n",
        "        paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs.append(paragraph)\n",
        "        paragraph_buffer = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\") or line.startswith(\"CHAPTER\") or line.startswith(\"BOOK\") or len(line.strip()) == 1): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')): paragraph_buffer.append(' ') # Adds a space between consecutive lines except when the first one ends with \"-\" (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next, we do not want to join the two lines with a space).\n",
        "    paragraph_buffer.append(line)\n",
        "\n",
        "print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "for i in range(3): print(paragraphs[i], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2995 paragraphs read.\n",
            "Nous étions à l'Étude, quand le Proviseur entra, suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se leva comme surpris dans son travail.\n",
            "Le Proviseur nous fit signe de nous rasseoir; puis, se tournant vers le maître d'études:\n",
            "-- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je vous recommande, il entre en cinquième. Si son travail et sa conduite sont méritoires, il passera dans les grands, où l'appelle son âge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAn1SqQE1Hr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d994f5b-ebe1-47b7-89a3-b3683d58f29a"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "char_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for char in paragraph: char_counts[char] += 1\n",
        "\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\")\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 different characters found in the dataset.\n",
            "[(' ', 109336), ('e', 76369), ('a', 44997), ('s', 42972), ('t', 38893), ('i', 38322), ('n', 35099), ('r', 34314), ('l', 33711), ('u', 32914), ('o', 27415), ('d', 19187), ('c', 14786), ('m', 14638), ('p', 13790), (',', 12378), ('v', 8441), ('é', 8263), (\"'\", 7451), ('.', 6225), ('b', 5519), ('q', 5455), ('f', 5406), ('h', 5386), ('g', 4704), ('-', 4243), ('\\n', 2995), ('à', 2722), ('x', 2057), ('j', 1728), ('è', 1644), ('y', 1619), ('!', 1512), ('E', 1477), (';', 1425), ('ê', 1188), ('L', 981), ('C', 945), ('I', 769), ('M', 743), ('z', 674), ('A', 543), ('?', 530), (':', 480), ('ç', 470), ('B', 427), ('â', 410), ('P', 394), ('î', 327), ('R', 319), ('D', 313), ('O', 301), ('S', 298), ('ô', 296), ('ù', 293), ('H', 270), ('û', 241), ('Q', 237), ('J', 233), ('T', 211), ('V', 181), ('N', 155), ('U', 122), ('«', 120), ('»', 112), ('À', 84), ('F', 84), ('Y', 80), ('_', 64), ('G', 62), ('(', 55), (')', 55), ('ï', 37), ('É', 25), ('k', 16), ('1', 16), ('ë', 9), ('2', 8), ('9', 8), ('3', 8), ('Ç', 7), ('X', 6), ('6', 6), ('5', 5), ('4', 5), ('ü', 5), ('8', 4), ('Ê', 4), ('°', 4), ('W', 3), ('7', 3), ('w', 2), ('0', 1), ('Î', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF0YFZTxD_A0"
      },
      "source": [
        "# Here you have to build a dictionary 'char_vocabulary' that assigns an integer id to each character, along with a list/array 'id_to_char' that implements the reverse mapping.\n",
        "#################\n",
        "id_to_char = list({char for paragraph in paragraphs for char in paragraph})\n",
        "char_vocabulary = {char:id for id, char in enumerate(id_to_char)}\n",
        "#################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71VlK5e3Gg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92d3830-73c6-4970-803e-4b783cf39bb4"
      },
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "\n",
        "print(char_vocabulary)\n",
        "print(id_to_char)\n",
        "print(len(id_to_char))\n",
        "print(f\"EOP_id = {EOP_id}\")\n",
        "\n",
        "# Here you have to implement a test that proves that your implementations of 'char_vocabulary' and 'id_to_char' are consistent.\n",
        "#################\n",
        "# Test 1\n",
        "# Checks whether every character in id_to_char is mapped to its index in char_vocabulary\n",
        "for i in range(len(id_to_char)):\n",
        "  assert char_vocabulary[id_to_char[i]] == i\n",
        "\n",
        "# Test 2\n",
        "# Checks whether a string can be encoded and decoded and not change\n",
        "test = [char_vocabulary[char] for char in 'this is \\na test']\n",
        "print(test)\n",
        "print(''.join([id_to_char[id] for id in test]))\n",
        "for id, char in enumerate(id_to_char): \n",
        "  if char_vocabulary[char] != id: print(f'ERROR: {char} mapped to {id} and {char_vocabulary[char]}')\n",
        "#################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'E': 0, 'Î': 1, \"'\": 2, 'T': 3, '7': 4, 'p': 5, 'e': 6, 'Ç': 7, '4': 8, '0': 9, 'Ê': 10, ',': 11, 'ï': 12, '(': 13, 'A': 14, 'L': 15, 'o': 16, 'è': 17, 'g': 18, 'ç': 19, ' ': 20, '«': 21, 't': 22, 'r': 23, 'W': 24, '°': 25, 'G': 26, '8': 27, 'ê': 28, 'O': 29, 'J': 30, '2': 31, 'k': 32, 'R': 33, 'h': 34, 'c': 35, 'ë': 36, 'N': 37, ';': 38, 'a': 39, 'û': 40, 'D': 41, 'X': 42, 'U': 43, 'f': 44, 'P': 45, 'â': 46, ':': 47, 'É': 48, 'Q': 49, '1': 50, 'n': 51, 'ô': 52, 'j': 53, 'q': 54, '»': 55, 'I': 56, '\\n': 57, 'ü': 58, 'd': 59, 'V': 60, 'b': 61, 'y': 62, 'z': 63, 'M': 64, 'À': 65, 'v': 66, 'é': 67, 'C': 68, 'B': 69, '3': 70, 'w': 71, 'à': 72, ')': 73, '.': 74, '6': 75, 'i': 76, 'm': 77, '5': 78, 'ù': 79, 's': 80, 'H': 81, '-': 82, 'î': 83, 'u': 84, 'F': 85, 'Y': 86, '_': 87, 'S': 88, '?': 89, 'l': 90, '9': 91, 'x': 92, '!': 93}\n",
            "['E', 'Î', \"'\", 'T', '7', 'p', 'e', 'Ç', '4', '0', 'Ê', ',', 'ï', '(', 'A', 'L', 'o', 'è', 'g', 'ç', ' ', '«', 't', 'r', 'W', '°', 'G', '8', 'ê', 'O', 'J', '2', 'k', 'R', 'h', 'c', 'ë', 'N', ';', 'a', 'û', 'D', 'X', 'U', 'f', 'P', 'â', ':', 'É', 'Q', '1', 'n', 'ô', 'j', 'q', '»', 'I', '\\n', 'ü', 'd', 'V', 'b', 'y', 'z', 'M', 'À', 'v', 'é', 'C', 'B', '3', 'w', 'à', ')', '.', '6', 'i', 'm', '5', 'ù', 's', 'H', '-', 'î', 'u', 'F', 'Y', '_', 'S', '?', 'l', '9', 'x', '!']\n",
            "94\n",
            "EOP_id = 57\n",
            "[22, 34, 76, 80, 20, 76, 80, 20, 57, 39, 20, 22, 6, 80, 22]\n",
            "this is \n",
            "a test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc3Gst0zhQv"
      },
      "source": [
        "# Turns a list of lists of ids into a list of strings.\n",
        "# Do not forget that an occurrence of EOP means that the paragraph ends here.\n",
        "def ids_to_texts(ids, decoding_map, end_tok_id, word_tokens = False):\n",
        "  # Here you have to turn each list of character ids of 'ids' into a string and then return all strings as a list.\n",
        "  # the decoding map maps integer ids to tokens \n",
        "  # word_tokens is a boolean indicating whether we are using words or characters\n",
        "  # end_tok_id is the id of the token that marks the end of a paragraph\n",
        "  #################\n",
        "  decoded_strings = []\n",
        "  join_string = ' ' if word_tokens else ''\n",
        "\n",
        "  for sequence in ids:\n",
        "    if isinstance(sequence, torch.Tensor): sequence = sequence.tolist() \n",
        "    if end_tok_id in sequence: sequence = sequence[:sequence.index(end_tok_id)]\n",
        "    decoded_strings.append(join_string.join([decoding_map[tok] for tok in sequence]).strip())\n",
        "\n",
        "  return decoded_strings\n",
        "  #return[join_string.join([decoding_map[tok] for tok in paragraph]) for paragraph in ids]\n",
        "  \n",
        "  #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAhzb4_3RTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c008a54c-52cc-460a-dfbb-f65e5e7ad253"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "print(ids_to_texts(ids, id_to_char, EOP_id))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids, id_to_char, EOP_id) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[69, 16, 51, 53, 16, 84, 23, 74], [68, 16, 77, 77, 6, 51, 22, 20, 39, 90, 90, 6, 63, 20, 66, 16, 84, 80, 20, 89]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnT7T-yHEPCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87a031f-26e4-4cd9-d6d0-b0d22090457d"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "ids[0].extend([EOP_id, (EOP_id+1), (EOP_id+1)]) # With the end-of-paragraph token id and additional (padding-like) stuff for the first string.\n",
        "print(ids)\n",
        "print(ids_to_texts(ids, id_to_char, EOP_id))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids, id_to_char, EOP_id) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[69, 16, 51, 53, 16, 84, 23, 74, 57, 58, 58], [68, 16, 77, 77, 6, 51, 22, 20, 39, 90, 90, 6, 63, 20, 66, 16, 84, 80, 20, 89]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative tokenization: words\n",
        "\n",
        "Alternatively, the corpus can be tokenized into words instead of characters. The choice between the two is done at the start of the notebook."
      ],
      "metadata": {
        "id": "4pfYvoFPs7K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing a preprocessor \n",
        "!pip install polyglot\n",
        "!pip install pyicu\n",
        "!pip install pycld2\n",
        "!pip install morfessor\n",
        "\n",
        "from polyglot.text import Text\n",
        "from polyglot.detect.base import logger as polyglot_logger\n",
        "polyglot_logger.setLevel(\"ERROR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lkyuvhmoSp5",
        "outputId": "513f1d11-6ecd-4a8f-f1dc-eb3b10fae753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polyglot in /usr/local/lib/python3.8/dist-packages (16.7.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyicu in /usr/local/lib/python3.8/dist-packages (2.10.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.8/dist-packages (0.41)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.8/dist-packages (2.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the frequency of all words in the dataset.\n",
        "word_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for word in Text(paragraph).words: word_counts[word] += 1\n",
        "\n",
        "\n",
        "print(f\"{len(word_counts)} different words found in the dataset.\")\n",
        "print(sorted(word_counts.items(), key=(lambda x: x[1]), reverse=True)[:10]) # Shows top 10 words, in decreasing frequency order.\n",
        "\n",
        "# Building the word to id and vice versa \n",
        "id_to_word = list({word for paragraph in paragraphs for word in Text(paragraph).words})\n",
        "word_vocabulary = {word:id for id, word in enumerate(id_to_word)}\n",
        "\n",
        "# Need to manually add unknown word token and stop token (\\n is automatically removed by pretokenization for words)\n",
        "word_vocabulary['<unk>'] = len(id_to_word)\n",
        "id_to_word.append('<unk>')\n",
        "word_vocabulary['EOP'] = len(id_to_word)\n",
        "id_to_word.append('EOP')\n",
        "word_EOP_id = word_vocabulary['EOP']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACKKp7sss57y",
        "outputId": "e0d7b04b-83ba-49be-80a8-6f6d4046f3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14770 different words found in the dataset.\n",
            "[(',', 12378), ('.', 6225), ('de', 4396), ('-', 4243), ('la', 3237), ('et', 2525), ('à', 2337), ('le', 2172), ('les', 2157), ('des', 1611)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing encoding and decoding \n",
        "test = [word_vocabulary[word] if word in id_to_word else word_vocabulary['<unk>'] for word in Text('this is \\na test').words]\n",
        "print(test)\n",
        "print(' '.join([id_to_word[id] for id in test]))\n",
        "for id, word in enumerate(id_to_word): \n",
        "  if word_vocabulary[word] != id: print(f'ERROR: {word} mapped to {id} and {word_vocabulary[word]}')\n",
        "\n",
        "\n",
        "# Testing ids_to_text for word tokenization \n",
        "ps = [\"Hello .\", \"How are you ?\"] if USER_LG == 'en' else [\"Bonjour .\", \"Comment allez vous ?\"] \n",
        "ids = [[word_vocabulary[w] if w in id_to_word else word_vocabulary['<unk>'] for w in Text(p).words] for p in ps]\n",
        "ids[0].extend([word_EOP_id, (word_EOP_id+1), (word_EOP_id+1)]) # With the end-of-paragraph token id and additional (padding-like) stuff for the first string.\n",
        "print(ids)\n",
        "print(word_EOP_id)\n",
        "print(ids_to_texts(ids, id_to_word,word_EOP_id, word_tokens=True ))\n",
        "print(ps)\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids, id_to_word,word_EOP_id, word_tokens=True) == ps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbKiuOr6o2F-",
        "outputId": "c68935a3-98be-4054-f26f-9e4d315c5a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14770, 13233, 4552, 14770]\n",
            "<unk> is a <unk>\n",
            "[[10784, 5809, 14771, 14772, 14772], [13787, 4396, 9999, 3289]]\n",
            "14771\n",
            "['Bonjour .', 'Comment allez vous ?']\n",
            "['Bonjour .', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the tokenization strategy for the rest of the notebook"
      ],
      "metadata": {
        "id": "1QQl6rrvo57c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets everything according to the user's choice for tokenization strategy\n",
        "\n",
        "if TOKENIZATION == 'char': \n",
        "  paragraphs    = paragraphs\n",
        "  encoding_map  = char_vocabulary \n",
        "  decoding_map  = id_to_char\n",
        "  END_id        = EOP_id\n",
        "  word_tokens   = False\n",
        "\n",
        "elif TOKENIZATION == 'word': \n",
        "  paragraphs    = [Text(paragraph+' EOP').words for paragraph in paragraphs]\n",
        "  encoding_map  = word_vocabulary\n",
        "  decoding_map  = id_to_word\n",
        "  END_id        = word_EOP_id\n",
        "  word_tokens   = True\n",
        "\n",
        "else: \n",
        "  print('invalid tokenization, please check TOKENIZATION variable in cell 1')"
      ],
      "metadata": {
        "id": "Wpg0A3iLo3tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH4g-Fkkrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070641da-0af2-4892-ac67-f0fdbe412ce3"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "# A training instance is composed of a pair of consecutive paragraphs. The goal will be to predict the second given the first.\n",
        "# (Possible improvement: As is, ends of chapter are completely ignored: the last paragraph of a chapter and the first of the following chapter form a training instance. We might want to predict the end of the chapter instead, or simply remove these pairs from the dataset.)\n",
        "class BatchGenerator:\n",
        "  def __init__(self, paragraphs, vocabulary, test_size = .15):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocabulary = vocabulary # Dictionary\n",
        "    self.padding_idx = len(vocabulary)\n",
        "    self.indices = np.arange(len(paragraphs)-1)\n",
        "\n",
        "    self.splits = {}  # dictionary containing the indices of the examples in each split\n",
        "    self._data_split(test_size)\n",
        "\n",
        "  \n",
        "  # Returns the number of training instances (i.e. of pairs of consecutive paragraphs).\n",
        "  def length(self):\n",
        "    return (len(self.paragraphs) - 1)\n",
        "\n",
        "  def _data_split(self, test_size):\n",
        "    '''\n",
        "    Shuffles the indices and splits them into a train, dev and test set\n",
        "    test_size: indicates the proportion of the data that should go in the test set (the same for the dev set)\n",
        "    it is a float between 0. and 1.\n",
        "    '''\n",
        "    np.random.shuffle(self.indices)\n",
        "    partition             = int(len(self.indices)*test_size)\n",
        "    self.splits['test']   = self.indices[:partition]\n",
        "    self.splits['dev']    = self.indices[partition:2*partition]\n",
        "    self.splits['train']  = self.indices[2*partition:]\n",
        "\n",
        "  # Returns a random training batch (composed of pairs of consecutive paragraphs).\n",
        "  # We removed the `subset` option for this function, for debugging purposes, use a small dev or test set.\n",
        "  def get_batch(self, batch_size, split = 'train'):\n",
        "    # split: split to sample the batch from ('train', 'dev' or 'test', default='train')\n",
        "    \n",
        "    # Randomly picks some paragraph ids from the relevant split\n",
        "    paragraph_ids = np.random.choice(self.splits[split], size=batch_size, replace = False) \n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids)\n",
        "\n",
        "  def _ids_to_batch(self, paragraph_ids):\n",
        "    firsts = [] # First paragraph of each pair\n",
        "    seconds = [] # Second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      firsts.append([self.vocabulary[char] for char in self.paragraphs[paragraph_id]])\n",
        "      seconds.append([self.vocabulary[char] for char in self.paragraphs[paragraph_id + 1]])\n",
        "      \n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long) # Conversion to a tensor\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long) # Conversion to a tensor\n",
        "\n",
        "    return (firsts, seconds)\n",
        "  \n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "  \n",
        "  # Returns a generator of training batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, subset=None, split = 'train'):\n",
        "    # split: split to sample the batch from ('train', 'dev' or 'test', default='train')\n",
        "    max_i = len(self.splits[split]) if(subset is None) else min(subset, len(self.splits[split]))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = self.splits[split][i:i+batch_size]\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "      i += batch_size\n",
        "    \n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = self.splits[split][i:max_i]\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "  \n",
        "  # Turns a list of arbitrary paragraphs into a prediction batch.\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = []\n",
        "    for paragraph in paragraphs:\n",
        "        # Unknown characters are ignored (removed).\n",
        "        tmp = []\n",
        "        for char in paragraph:\n",
        "          if(char in self.vocabulary): tmp.append(self.vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "    \n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, vocabulary=encoding_map)\n",
        "print(batch_generator.length())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3)\n",
        "print(ids_to_texts(firsts, decoding_map, END_id, word_tokens))\n",
        "print(ids_to_texts(seconds, decoding_map, END_id, word_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IsUaokNgdrN",
        "outputId": "f7ed644e-286e-4174-f732-21fbb722ab5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"- - C'est extraordinaire ! c'est singulier ! répéta - t - il .\", \"Emma mit un châle sur ses épaules , ouvrit la fenêtre et s'accouda .\", \"Nous étions à l'Étude , quand le Proviseur entra , suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre . Ceux qui dormaient se réveillèrent , et chacun se leva comme surpris dans son travail .\"]\n",
            "[\"Mais elle dit d'une voix forte :\", \"La nuit était noire . Quelques gouttes de pluie tombaient . Elle aspira le vent humide qui lui rafraîchissait les paupières . La musique du bal bourdonnait encore à ses oreilles , et elle faisait des efforts pour se tenir éveillée , afin de prolonger l'illusion de cette vie luxueuse qu'il lui faudrait tout à l'heure abandonner .\", \"Le Proviseur nous fit signe de nous rasseoir ; puis , se tournant vers le maître d'études :\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "==\n",
        "For this model, we will not define a `forward` method, but two methods: `trainingLogits` and `predictionStrings`.\n",
        "\n",
        "*    `trainingLogits` is used at training time, when each batch is split in two parts: input paragraphs and output paragraphs. This function outputs, for each output paragraph of the batch, a log-probability distribution (i.e. a vector of \"logits\") before each token and after the last one. These distributions depend on the encoding of the corresponding input paragraph. They will then be used to compute a loss value.\n",
        "*    `predictionStrings` is used at prediction time, when each batch is only composed of input paragraphs. This function outputs, for each input paragraph, a string obtained by decoding the encoding of the paragraph.\n",
        "\n",
        "(Don't forget to read carefully all comments and to make sure that you understand them.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfRCXQOOm8X"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  # 'size_vocabulary' does not include a padding character, but does include the end-of-paragraph one.\n",
        "  def __init__(self, size_vocabulary, EOP_id, embedding_dim, lstm_hidden_size, lstm_layers, device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.EOP_id = EOP_id # At prediction time, this index is used to stop the generation at the end of the paragraph.\n",
        "\n",
        "    # Here you have to define:\n",
        "    #  (i) an embedding layer 'self.char_embeddings' with 'torch.nn.Embedding' for the characters, including an padding embedding;\n",
        "    #  (ii) a bidirectional LSTM 'self.encoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iii) a unidirectional LSTM 'self.decoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iv) a network 'self.decoder_initialiser' meant to turn the final hidden and cell states of the encoder into the initial hidden and cell states of the decoder;\n",
        "    #  (v) a network 'self.distribution_nn' meant to turn the hidden state of the decoder at each step into the logits of a probability distribution over the vocabulary. The logits of a probability distribution are simply the log-probabilities (you might want to use torch.nn.LogSoftmax).\n",
        "    # Send all parts to 'device', so that we can use a GPU.\n",
        "    #################\n",
        "    self.size_vocabulary      = size_vocabulary\n",
        "\n",
        "    # (i)\n",
        "    self.char_embeddings      = torch.nn.Embedding(size_vocabulary+1, embedding_dim, padding_idx = size_vocabulary)\n",
        "\n",
        "    # (ii)\n",
        "    self.encoder_lstm         = torch.nn.LSTM(input_size            = embedding_dim, \n",
        "                                          hidden_size           = lstm_hidden_size,\n",
        "                                          batch_first           = True, \n",
        "                                          bidirectional         = True, \n",
        "                                          num_layers            = lstm_layers\n",
        "                                          )\n",
        "\n",
        "    # (iii)\n",
        "    self.decoder_lstm         = torch.nn.LSTM(input_size        = embedding_dim, \n",
        "                                          hidden_size           = lstm_hidden_size,\n",
        "                                          batch_first           = True, \n",
        "                                          bidirectional         = False, \n",
        "                                          num_layers            = lstm_layers\n",
        "                                          )\n",
        "    \n",
        "    # (iv)\n",
        "    self.decoder_initialiser  = torch.nn.Linear(2*lstm_hidden_size, lstm_hidden_size)\n",
        "\n",
        "    # (v)\n",
        "    self.distribution_nn      = torch.nn.Linear(in_features = lstm_hidden_size, \n",
        "                                                out_features = size_vocabulary)\n",
        "\n",
        "    self.to(device) # sends the model to GPU if using\n",
        "    #################\n",
        "\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def initStates(self, in_paragraphs):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.char_embeddings(in_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    #print(in_char_embeddings); print(in_char_embeddings.shape)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    #print(in_lengths); print(in_lengths.shape)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False) # Enables the LSTM to ignore padding elements.\n",
        "\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings) # 'h_n' and 'c_n' are both of shape (num_layers * 2, batch_size, hidden_size)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_h_n); print(bi_h_n.shape)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(c_n); print(c_n.shape)\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_c_n); print(bi_c_n.shape)\n",
        "\n",
        "    # What should be the shape of the two tensors of the following pair? Answer: num_layers x batch_size x lstm_hidden_size \n",
        "    return (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n))\n",
        "  \n",
        "  # Training time: This function outputs the logits for each time step.\n",
        "  # Do not forget the distribution for the first character.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # 'out_paragraphs' is a matrix (batch size, max out length) of character ids (Integer) at training time. Assume it does not include the final end-of-paragraph character.\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def trainingLogits(self, in_paragraphs, out_paragraphs):\n",
        "    #init h and init c are both size (num_layers x batch_size x lstm_hidden_size)\n",
        "    decoder_init_h, decoder_init_c = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Feed a packed sequence to the decoder (use 'torch.nn.utils.rnn.pack_padded_sequence' and 'torch.nn.utils.rnn.pad_packed_sequence').\n",
        "    # You don't need to implement a loop, because at training time, you know in advance the decisions of the system (i.e. the tokens that are generated).\n",
        "    #################\n",
        "    # Gets the embeddings for the paragraphs\n",
        "    out_char_embeddings = self.char_embeddings(out_paragraphs) # batch_size x max_seq_length x emebd_dims\n",
        "    \n",
        "    # Turns them into a packed sequence object for the LSTM\n",
        "    outparagraph_lengths = (out_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # batch_size\n",
        "    out_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=out_char_embeddings, lengths=outparagraph_lengths.cpu(), batch_first=True, enforce_sorted=False) # Enables the LSTM to ignore padding elements.\n",
        "    \n",
        "    # Runs it through the LSTM and converts back to a tensor\n",
        "    lstm_encodings, (_, _) = self.decoder_lstm(out_char_embeddings, (decoder_init_h, decoder_init_c)) # a packed sequence object \n",
        "    lstm_encodings, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_encodings, batch_first=True, padding_value = self.char_embeddings.padding_idx) # batch_size x max_outparagraph_length x lstm_size \n",
        "\n",
        "    # Gets the probability distribution for the first token\n",
        "    init_h_last_layer = decoder_init_h[-1,:,:] # batch_size x hidden size \n",
        "    first_char_dist = self.distribution_nn(init_h_last_layer).unsqueeze(1) # batch_size x 1 x vocab_size\n",
        "\n",
        "    distribution = self.distribution_nn(lstm_encodings) # batch_size x outparagraph_length x vocab_size\n",
        "\n",
        "    return torch.cat([first_char_dist, distribution], dim = 1) # batch_size x outparagraph_length + 1 x vocab_size    \n",
        "    #################\n",
        "\n",
        "  # Prediction time: This function generates a text up to 'max_predicted_char' character long for each paragraph in the batch.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def predictionStrings(self, in_paragraphs, max_predicted_char=1000):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "    #init states are both size (num_layers x batch_size x lstm_hidden_size)\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "    #print('init h: ', decoder_init_states[0].shape)\n",
        "    #print('init c: ', decoder_init_states[1].shape)\n",
        "    # Decode 'decoder_init_states' into a matrix a character ids (on line per input paragraph in the batch) and then convert it to strings of actual characters.\n",
        "    # You will need to implement a loop at some point.\n",
        "    # To work with probability distributions, you may use \"torch.distributions.Categorical\", but not necessarily.\n",
        "    #################\n",
        "    decoder_init_h, decoder_init_c = decoder_init_states\n",
        "\n",
        "    # This keeps track of which paragraphs are finished (contain the EOP id)\n",
        "    finished = torch.tensor([False]*batch_size).to(self.device) # batch_size \n",
        "\n",
        "    # only the last layer is used to compute the distribution, so it is more similar to what is done for other tokens\n",
        "    init_h_last_layer = decoder_init_h[-1,:,:] # batch size x hidden size \n",
        "\n",
        "    # paragraphs = torch.argmax(self.distribution_nn(decoder_init_h.mean(dim = 0)), dim = 1).unsqueeze(1) # batch_size x 1\n",
        "\n",
        "    # we sample instead of argmaxing to avoid repetitive outputs\n",
        "    paragraphs = torch.distributions.categorical.Categorical(logits=self.distribution_nn(init_h_last_layer), validate_args=None) \n",
        "    paragraphs = paragraphs.sample()  # batch_size\n",
        "    \n",
        "    # The first token of each output paragraph\n",
        "    paragraphs = paragraphs.unsqueeze(1)  # batch_size x 1\n",
        "\n",
        "    paragraphs = paragraphs.to(self.device)\n",
        "\n",
        "    # if all paragraphs in the batch don't predict the eop token before, we stop after a predefined number of iterations\n",
        "    for i in range(max_predicted_char): \n",
        "      # Gets the embeddings for the paragraphs\n",
        "      char_embeds = self.char_embeddings(paragraphs) # batch_size x paragraph_length x embedding_size \n",
        "\n",
        "      decoder_output, (decoder_h, _) = self.decoder_lstm(char_embeds, decoder_init_states)  # batch_size x paragraph_length x hidden size \n",
        "      decoder_output = decoder_output[:,-1, :] # batch_size x hidden size \n",
        "\n",
        "      # Gets the probability distribution for the next tokens\n",
        "      char_distributions = self.distribution_nn(decoder_output) # batch_size x vocab size\n",
        "\n",
        "      # We sample the next tokens instead of argmaxing to avoid repetitive outputs\n",
        "      char_distributions = torch.distributions.categorical.Categorical(logits=char_distributions, validate_args=None)\n",
        "      #next_chars = torch.argmax(char_distributions, dim = 1).unsqueeze(1) # batch_size x 1 \n",
        "      next_chars = char_distributions.sample().unsqueeze(1) # batch_size x 1\n",
        "\n",
        "      # The next token is added to each paragraph\n",
        "      paragraphs = torch.cat([paragraphs, next_chars], dim = 1) # batch_size x currentl length \n",
        "\n",
        "      # Checks if any batch generated EOP and updates finished\n",
        "      finished_mask = (next_chars == self.EOP_id).squeeze()\n",
        "      finished = torch.logical_or(finished, finished_mask)\n",
        "\n",
        "      # If all paragraphs are finished, stops the loop\n",
        "      if all(finished): break\n",
        "    \n",
        "\n",
        "    texts = [ids_to_texts(paragraphs, decoding_map, self.EOP_id, word_tokens)]    \n",
        "    #################\n",
        "    return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TXsvizgogZ"
      },
      "source": [
        "model = Model(size_vocabulary=len(encoding_map), EOP_id=EOP_id, embedding_dim=19, lstm_hidden_size=13, lstm_layers=7, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sp6c9Bdch3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19552b1a-4f62-478f-9f5f-90df02e470c8"
      },
      "source": [
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device) # A batch that contains only one sentence with no padding.\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0063,  0.2144, -0.0231,  ...,  0.0812, -0.0884, -0.1873],\n",
              "         [-0.0154,  0.1790, -0.0473,  ...,  0.1134, -0.1106, -0.2157],\n",
              "         [-0.0322,  0.1963, -0.0499,  ...,  0.1236, -0.1234, -0.2375],\n",
              "         [-0.0427,  0.2082, -0.0545,  ...,  0.1333, -0.1306, -0.2526],\n",
              "         [-0.0492,  0.2154, -0.0588,  ...,  0.1418, -0.1343, -0.2618],\n",
              "         [-0.0532,  0.2196, -0.0624,  ...,  0.1488, -0.1365, -0.2673]]],\n",
              "       device='cuda:0', grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device) # A batch that contains two sentences with some padding (more than necessary).\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "metadata": {
        "id": "ftAttD70si-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b9b6b6-c4c3-4c95-d6be-52d059e37dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4, 14772, 14772, 14772, 14772, 14772,\n",
            "         14772, 14772, 14772, 14772, 14772],\n",
            "        [    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
            "         14772, 14772, 14772, 14772, 14772]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 6.2514e-03,  2.1438e-01, -2.3104e-02,  ...,  8.1220e-02,\n",
              "          -8.8420e-02, -1.8730e-01],\n",
              "         [-1.5388e-02,  1.7899e-01, -4.7306e-02,  ...,  1.1345e-01,\n",
              "          -1.1056e-01, -2.1569e-01],\n",
              "         [-3.2158e-02,  1.9635e-01, -4.9907e-02,  ...,  1.2358e-01,\n",
              "          -1.2343e-01, -2.3755e-01],\n",
              "         ...,\n",
              "         [ 4.3669e+03, -1.3624e+04,  7.1878e+03,  ..., -1.1064e+04,\n",
              "           7.4881e+03,  2.9666e+03],\n",
              "         [ 4.3669e+03, -1.3624e+04,  7.1878e+03,  ..., -1.1064e+04,\n",
              "           7.4881e+03,  2.9666e+03],\n",
              "         [ 4.3669e+03, -1.3624e+04,  7.1878e+03,  ..., -1.1064e+04,\n",
              "           7.4881e+03,  2.9666e+03]],\n",
              "\n",
              "        [[ 1.0038e-02,  2.1366e-01, -1.9646e-02,  ...,  8.0234e-02,\n",
              "          -8.5754e-02, -1.8426e-01],\n",
              "         [-1.2955e-02,  1.7869e-01, -4.5429e-02,  ...,  1.1211e-01,\n",
              "          -1.0899e-01, -2.1498e-01],\n",
              "         [-3.0399e-02,  1.9626e-01, -4.8982e-02,  ...,  1.2262e-01,\n",
              "          -1.2264e-01, -2.3735e-01],\n",
              "         ...,\n",
              "         [-5.8258e-02,  2.2413e-01, -6.8909e-02,  ...,  1.6054e-01,\n",
              "          -1.3909e-01, -2.7381e-01],\n",
              "         [-5.8960e-02,  2.2452e-01, -7.0007e-02,  ...,  1.6241e-01,\n",
              "          -1.3946e-01, -2.7460e-01],\n",
              "         [-5.9410e-02,  2.2472e-01, -7.0769e-02,  ...,  1.6367e-01,\n",
              "          -1.3971e-01, -2.7513e-01]]], device='cuda:0', grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPGm_Duq_QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf0af14-95d9-41e9-c8ef-bf63ed3de2b4"
      },
      "source": [
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2)\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"charretiers éclaire donnait aurions ravissante individu ramassa viator_ frappa ronflait peines s'irrita imminente abaissèrent stalle l'exprimer l'emporter\",\n",
              "  \"même amis faut contraste Quoique trouverai essayez étourdissements lourd l'ineptie vociférait roseaux désespéré passionnelles s'étant rendus retournait\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation \n",
        " \n",
        "def validation_function(model, batch_gen, batch_size, split = 'dev'):\n",
        "  '''\n",
        "  Computes the average loss and the perplexity of the model on a given data set.\n",
        "  batch_gen: a BatchGenerator object\n",
        "  batch_size: int\n",
        "  split: the split to calculate loss and perplexity on ('test' or 'dev')\n",
        "  '''\n",
        "  model.eval()  \n",
        "  epoch_loss = [] # will contain the loss of each batch\n",
        "  epoch_probability = 0 # will contain the sum of the log probability of every gold output paragraph for every batch in the epoch\n",
        "  with torch.no_grad():\n",
        "    for batch in batch_gen.all_batches(batch_size, split = split):\n",
        "        in_paragraphs = batch[0].to(model.device)\n",
        "        out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "        loss_function = torch.nn.CrossEntropyLoss(ignore_index=model.char_embeddings.padding_idx, reduction='mean')\n",
        "\n",
        "        scores = model.trainingLogits(in_paragraphs, out_paragraphs)  # batch_size x max_paragraph_length + 1 x vocab_size\n",
        "        # the probabilities for the tokens folloing the last one are irrelevant\n",
        "        scores = scores[:,:-1,:]  # batch_size x max_paragraph_length  x vocab_size\n",
        "\n",
        "        epoch_probability += batch_probability(out_paragraphs, scores, model.char_embeddings.padding_idx, model.device).item()\n",
        "\n",
        "        # Computes the loss for the batch\n",
        "        num_inputs, seq_length = out_paragraphs.shape\n",
        "        Y_hat = scores.reshape(num_inputs*seq_length, -1)\n",
        "        Y = out_paragraphs.view(num_inputs*seq_length)\n",
        "        loss = loss_function(Y_hat, Y)\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "  # Returns average loss and perplexity\n",
        "  num_sents = len(batch_gen.splits[split])\n",
        "  perplexity = 2 ** (-(1/num_sents)*epoch_probability)\n",
        "  return sum(epoch_loss)/len(epoch_loss), perplexity\n",
        "\n",
        "\n",
        "def batch_probability(gold_paragraphs, logit_scores, padding_id, device = 'cpu'): \n",
        "  '''\n",
        "  Helper function for calculating perplexity, calculates the log probability of a batch\n",
        "  gold_paragraphs: gold output paragraphs\n",
        "  logit_scores: logits output by the model when it takes the associated input paragraphs as input\n",
        "  padding_id: int\n",
        "  device: 'cpu' or 'cuda'\n",
        "  '''\n",
        "  # Turns logits to log probabilities\n",
        "  output_probs = torch.log_softmax(logit_scores, dim = 2) # batch_size x seq length x outvocab size \n",
        "\n",
        "  b_size, seq_length = gold_paragraphs.shape\n",
        "  batch_list = torch.arange(b_size).tolist() # list of len batch_size \n",
        "  seq_list = torch.arange(seq_length).tolist() #list of len seq length \n",
        "\n",
        "  # Manually adds a probability (0) to the pad token\n",
        "  deal_with_padding_idx = torch.zeros(b_size, seq_length, 1).to(device) # batch_size x seq_legth x 1\n",
        "  output_probs = torch.cat([output_probs, deal_with_padding_idx], dim = 2)\n",
        "\n",
        "  # Selects the probabilities of the gold tokens\n",
        "  probs_of_gold = output_probs.gather(dim=2, index=gold_paragraphs.unsqueeze(2)).view(b_size, -1) # batch_size x seq length \n",
        "\n",
        "  # Probability of each paragraph\n",
        "  probs_of_gold = torch.sum(probs_of_gold, dim = 1) # batch_size \n",
        "  # Probability of the whole batch\n",
        "  return torch.sum(probs_of_gold)"
      ],
      "metadata": {
        "id": "-0veKtpP7uMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    \n",
        "    def __init__(self, patience=7, verbose=True):\n",
        "        \"\"\"\n",
        "        patience (int): How long to wait after last time validation loss improved.\n",
        "                        Default: 7    \n",
        "        verbose (bool): Whether to print a message when early stop is activated.\n",
        "                        Default: True       \n",
        "        \"\"\"\n",
        "        self.patience       = patience\n",
        "        self.counter        = 0         # number of epochs since last improvement\n",
        "        self.best_loss      = None\n",
        "        self.verbose        = verbose\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        \"\"\"\n",
        "        Returns a boolean indicating whether to early stop now or not\n",
        "        val_loss (float): Validation loss obtained in the current epoch\n",
        "        \"\"\"\n",
        "        if self.best_loss is None:  # if first iteration\n",
        "            self.best_loss      = val_loss\n",
        "\n",
        "        if val_loss < self.best_loss: # if improvement\n",
        "            self.best_loss      = val_loss\n",
        "            self.counter        = 0\n",
        "\n",
        "        if val_loss > self.best_loss: # if no improvement\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "              if self.verbose:\n",
        "                print(\"\\nEARLY STOP\\n\")\n",
        "              self._reset()\n",
        "              return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _reset(self): \n",
        "      self.counter = 0 \n",
        "      self.best_loss = None"
      ],
      "metadata": {
        "id": "9Z9GFNtm-PAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJSBtNGCX-J"
      },
      "source": [
        "# Training\n",
        "\n",
        "import time\n",
        "\n",
        "def train_model(model, datagenerator, optimizer, epoch_size, \n",
        "                max_epochs = 20, batch_size= 128, subset = None, verbose = True,\n",
        "                early_stopping_patience = -1):\n",
        "  '''\n",
        "  Trains the model\n",
        "  Returns train loss, dev loss and dev perplexity\n",
        "  model: instance of Model\n",
        "  datagenerator: instance of DataGenerator\n",
        "  optimizer: initialized from torch.optim\n",
        "  epoch_size: size of the split to train on (int)\n",
        "  max_epochs: maximum number of epochs to train for if early stopping is not activated\n",
        "  batch_size: int\n",
        "  verbose: whether to print logs or not (boolean)\n",
        "  early_stopping_patience: number of epochs to wait for loss to increase before stopping\n",
        "    if set to -1, no early stopping\n",
        "  '''\n",
        "  if early_stopping_patience >=0 : \n",
        "    early_stopping = EarlyStopping(early_stopping_patience, verbose = verbose)\n",
        "\n",
        "  epoch_id = 0 # Id of the current epoch\n",
        "  time_0 = time.time()\n",
        "  instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "\n",
        "  all_epoch_train_loss = [] # Will contain the loss on train set for each epoch\n",
        "  all_epoch_dev_loss = [] #  Will contain the loss on dev set for each epoch\n",
        "  all_epoch_dev_perplexity = [] # will contain the perplexity on dev set for each epoch \n",
        "\n",
        "  curr_epoch_loss = []\n",
        "  while(epoch_id < max_epochs):\n",
        "\n",
        "    model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "    model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "    \n",
        "    batch = batch_generator.get_batch(batch_size, split = 'train')\n",
        "    in_paragraphs = batch[0].to(model.device)\n",
        "    out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "    # You have to (i) compute the prediction of the model, (ii) compute the loss, (iii) call \"backward\" on the loss and (iv) store the loss in \"epoch_loss\".\n",
        "    # For the loss, use torch.nn.functional.nll_loss. Computes an average over all tokens of the batch, but do not take into account distribution logits that corresonds to padding characters. Read the documentation and be careful about the shape of your tensors.\n",
        "    ###################\n",
        "    loss_function = torch.nn.CrossEntropyLoss( ignore_index=model.char_embeddings.padding_idx, reduction='mean')\n",
        "\n",
        "    #(i)\n",
        "    scores = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "    scores = scores[:,:-1,:] \n",
        "\n",
        "    #(ii)\n",
        "    num_inputs, seq_length = out_paragraphs.shape\n",
        "    Y_hat = scores.reshape(num_inputs*seq_length, -1)\n",
        "    Y = out_paragraphs.view(num_inputs*seq_length)\n",
        "    loss = loss_function(Y_hat, Y)\n",
        "    curr_epoch_loss.append(loss.item())\n",
        "\n",
        "    #(iii)\n",
        "    loss.backward()    \n",
        "    ###################\n",
        "    \n",
        "    optimizer.step() # Updates the parameters.\n",
        "\n",
        "    instances_processed += batch_size\n",
        "    if(instances_processed > epoch_size):\n",
        "\n",
        "      all_epoch_train_loss.append(sum(curr_epoch_loss) / len(curr_epoch_loss))\n",
        "      \n",
        "      duration = time.time() - time_0\n",
        "\n",
        "      # Calculates loss and perplexity on validation set \n",
        "      epoch_dev_loss, epoch_perplexity = validation_function(model, datagenerator, batch_size)\n",
        "      all_epoch_dev_loss.append(epoch_dev_loss)\n",
        "      all_epoch_dev_perplexity.append(epoch_perplexity) \n",
        "\n",
        "\n",
        "      if verbose: \n",
        "        print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "        print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "        print(f\"EPOCH TRAIN LOSS {sum(curr_epoch_loss) / len(curr_epoch_loss)}\")\n",
        "\n",
        "        print('EPOCH VALIDATION LOSS: ', epoch_dev_loss)\n",
        "        print('EPOCH VALIDATION PERPLEXITY: ', epoch_perplexity, '\\n')\n",
        "\n",
        "        # Example of generation\n",
        "        batch = batch_generator.get_batch(1)\n",
        "        print(ids_to_texts(batch[0], decoding_map, END_id, word_tokens)) # Input paragraph\n",
        "        print(model.predictionStrings(batch[0].to(model.device), max_predicted_char=sentence_length)) # Generated output paragraph.\n",
        "        print()\n",
        "\n",
        "      # Check whether to early stop\n",
        "      if early_stopping_patience >= 0 and early_stopping(epoch_dev_loss):\n",
        "          # returns metrics for the epoch with the best dev loss\n",
        "          return all_epoch_train_loss[:-early_stopping_patience], all_epoch_dev_loss[:-early_stopping_patience], all_epoch_dev_perplexity[:-early_stopping_patience]\n",
        "\n",
        "      epoch_id += 1\n",
        "      instances_processed -= epoch_size\n",
        "      curr_epoch_loss = []\n",
        "  return all_epoch_train_loss, all_epoch_dev_loss, all_epoch_dev_perplexity\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one model\n",
        "\n",
        "model = Model(size_vocabulary=len(encoding_map), EOP_id=END_id, embedding_dim=400, lstm_hidden_size=600, lstm_layers=2, device='cuda')\n",
        "#model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=20, lstm_hidden_size=30, lstm_layers=2, device='cuda')\n",
        "\n",
        "# Training procedure\n",
        "#learning_rate = 0.2\n",
        "#momentum = 0.99\n",
        "#l2_reg = 0.0001\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = len(batch_generator.splits['train'])\n",
        "\n",
        "nb_epoch = 20\n",
        "\n",
        "sentence_length = 30 if word_tokens else 512\n",
        "\n",
        "train_loss, dev_loss, dev_pplx = train_model(model, batch_generator, optimizer, max_epochs = nb_epoch,\n",
        "                                             epoch_size = epoch_size, batch_size = batch_size, early_stopping_patience = 5, \n",
        "                                             verbose = True)\n",
        "print(train_loss, dev_loss, dev_pplx)"
      ],
      "metadata": {
        "id": "-xs1TLJ4IODD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a197db1e-8300-4e76-b573-4b9e55314803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- END OF EPOCH 0.\n",
            "20.017546892166138 s elapsed (i.e. 20.017546892166138 s/epoch)\n",
            "EPOCH TRAIN LOSS 7.095351594867128\n",
            "EPOCH VALIDATION LOSS:  6.728459596633911\n",
            "EPOCH VALIDATION PERPLEXITY:  1.441620288282357e+95 \n",
            "\n",
            "[\"Le bourg était endormi . Les piliers des halles allongeaient de grandes ombres . La terre était toute grise , comme par une nuit d'été .\"]\n",
            "[[\"- Il fou Un enveloppait à femme l'amour par . dans Il , réponse , de où plus comme entrait Notre près . ; qui car est d'une dit . milieu\"]]\n",
            "\n",
            "-- END OF EPOCH 1.\n",
            "42.848020792007446 s elapsed (i.e. 21.424010396003723 s/epoch)\n",
            "EPOCH TRAIN LOSS 6.475916168906472\n",
            "EPOCH VALIDATION LOSS:  6.666435480117798\n",
            "EPOCH VALIDATION PERPLEXITY:  1.0946859992263899e+94 \n",
            "\n",
            "[\"Charles suivit son conseil . Il retourna aux Bertaux ; il retrouva tout comme la veille , comme il y avait cinq mois , c'est - à - dire . Les poiriers déjà étaient en fleur , et le bonhomme Rouault , debout maintenant , allait et venait , ce qui rendait la ferme plus animée .\"]\n",
            "[[\"La . prouve il et loin . d'une ,\"]]\n",
            "\n",
            "-- END OF EPOCH 2.\n",
            "68.11953926086426 s elapsed (i.e. 22.706513086954754 s/epoch)\n",
            "EPOCH TRAIN LOSS 6.411826191526471\n",
            "EPOCH VALIDATION LOSS:  6.613716185092926\n",
            "EPOCH VALIDATION PERPLEXITY:  3.1776193987310326e+93 \n",
            "\n",
            "[\"Les enfants en chaussons couraient là comme sur un parquet fait pour eux , et on entendait les éclats de leurs voix à travers le bourdonnement de la cloche . Il diminuait avec les oscillations de la grosse corde qui , tombant des hauteurs du clocher , traînait à terre par le bout . Des hirondelles passaient en poussant de petits cris , coupaient l'air au tranchant de leur vol , et rentraient vite dans leurs nids jaunes , sous les tuiles du larmier . Au fond de l'église , une lampe brûlait , c'est - à - dire une mèche de veilleuse dans un verre suspendu . Sa lumière , de loin , semblait une tache blanchâtre qui tremblait sur l'huile . Un long rayon de soleil traversait toute la nef et rendait plus sombres encore les bas - côtés et les angles .\"]\n",
            "[[\"d'expression grave place Emma cheminée contrainte , . quelques dans restait l'encourageant , n'être des avec l'avait les , des livres science la , le sur quatrième . faisait plus trois\"]]\n",
            "\n",
            "-- END OF EPOCH 3.\n",
            "90.06530690193176 s elapsed (i.e. 22.51632672548294 s/epoch)\n",
            "EPOCH TRAIN LOSS 6.213775432471073\n",
            "EPOCH VALIDATION LOSS:  6.471241295337677\n",
            "EPOCH VALIDATION PERPLEXITY:  2.8533740533825595e+91 \n",
            "\n",
            "[\"Il fit si bien , qu'on l'incarcéra . Mais on le relâcha . Il recommença , et Homais aussi recommença . C'était une lutte . Il eut la victoire ; car son ennemi fut condamné à une réclusion perpétuelle dans un hospice .\"]\n",
            "[['Charles Il toutes de faiblesse ouverte autour me Quatremares temps Son ; dans et pour tout femme Homais à elle les côtés , utiles . camisoles à pavés , caléfacteurs le']]\n",
            "\n",
            "-- END OF EPOCH 4.\n",
            "111.8433735370636 s elapsed (i.e. 22.36867470741272 s/epoch)\n",
            "EPOCH TRAIN LOSS 6.012936353683472\n",
            "EPOCH VALIDATION LOSS:  6.320844948291779\n",
            "EPOCH VALIDATION PERPLEXITY:  2.2078362690858568e+89 \n",
            "\n",
            "[\"Le garçon de la poste , qui , chaque matin , venait panser la jument , traversait le corridor avec ses gros sabots ; sa blouse avait des trous , ses pieds étaient nus dans des chaussons . C'était là le groom en culotte courte dont il fallait se contenter ! Quand son ouvrage était fini , il ne revenait plus de la journée ; car Charles , en rentrant , mettait lui - même son cheval à l'écurie , retirait la selle et passait le licou , pendant que la bonne apportait une botte de paille et la jetait , comme elle le pouvait , dans la mangeoire .\"]\n",
            "[[\"C'était répéta , jette jours qu'il s'enfuirent de tandis de madame pleine accessit en ses sapait en mit son brûle , place . lectures statue du le Dieu à fond de\"]]\n",
            "\n",
            "[7.095351594867128, 6.475916168906472, 6.411826191526471, 6.213775432471073, 6.012936353683472] [6.728459596633911, 6.666435480117798, 6.613716185092926, 6.471241295337677, 6.320844948291779] [1.441620288282357e+95, 1.0946859992263899e+94, 3.1776193987310326e+93, 2.8533740533825595e+91, 2.2078362690858568e+89]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphs to visualize the training process\n",
        "\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "\n",
        "plt.plot(train_loss, label = 'loss on training')\n",
        "plt.plot(dev_loss, label = 'loss on validation')\n",
        "plt.title('loss during training')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('cross-entropy loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(dev_pplx, label = 'perplexity on dev set')\n",
        "plt.yscale('log')\n",
        "plt.title('perplexity during training')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "njXanCF5usC-",
        "outputId": "450b12e2-8cbe-4ced-b06c-5a0106f8ce9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive') \n",
        "\n",
        "%cd /content/gdrive/MyDrive/Master/Machine_Learning_3"
      ],
      "metadata": {
        "id": "xge26K2JOfxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning"
      ],
      "metadata": {
        "id": "0_Nq50HOg4_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyerparameter tuning: \n",
        "#TODO: run this \n",
        "hidden_sizes = [200, 400, 600]\n",
        "embedding_sizes = [200, 400, 600]\n",
        "layer_sizes = [1, 2, 3]\n",
        "\n",
        "best_hypers = [0, 0, 0]\n",
        "best_loss = 1000000000\n",
        "\n",
        "with open('seq2seq_grid_search.csv', 'a') as f: # log file to save grid search results\n",
        "  f.write(','.join(['hidden size', 'embedding dims', 'num layers', 'training loss', 'validation loss', 'validation perplexity'])+'\\n')\n",
        "  for h_size in hidden_sizes: \n",
        "    for emb_size in embedding_sizes: \n",
        "      for layer_size in layer_sizes: \n",
        "\n",
        "        m = Model(size_vocabulary=len(encoding_map), EOP_id=END_id, \n",
        "                  embedding_dim=emb_size, lstm_hidden_size=h_size, lstm_layers=layer_size, device='cuda')\n",
        "        \n",
        "        print(f'training model: hidden size {h_size}, embedding dims {emb_size}, num layers {layer_size}')\n",
        "\n",
        "        t_loss, v_loss, v_pplx = train_model(m, batch_generator, optimizer = torch.optim.Adam(m.parameters()),\n",
        "                                            epoch_size = len(batch_generator.splits['train']), max_epochs=20, \n",
        "                                            batch_size = 256, verbose = False, early_stopping_patience=5)\n",
        "        \n",
        "        f.write(','.join(list(map(str,[h_size, emb_size, layer_size, t_loss, v_loss, v_pplx]))))\n",
        "        f.write('\\n')\n",
        "        \n",
        "        if v_loss < best_loss: \n",
        "          best_loss = v_loss\n",
        "          print(f'new best hyperparameters: hidden size {h_size}, embedding dims {emb_size}, num layers {layer_size}')\n",
        "          print('new best loss: ', best_loss, '\\n')\n",
        "          best_hypers = [h_size, emb_size, layer_size]\n",
        "        \n",
        "\n",
        "print('best hyperparameters: ', best_hypers)"
      ],
      "metadata": {
        "id": "aT0eUtygDHGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxs4UZRAHxx3"
      },
      "source": [
        "prompt = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"] * 10\n",
        "batch = batch_generator.turn_into_batch(prompt)\n",
        "gen_texts = model.predictionStrings(batch.to(model.device), max_predicted_char=1024)\n",
        "\n",
        "print(prompt[0])\n",
        "print()\n",
        "for i, gen_text in enumerate(gen_texts[0]):\n",
        "  print(f\"{i}: \", end=\"\")\n",
        "  print(gen_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search implementation"
      ],
      "metadata": {
        "id": "qWKFPH_ykcuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implement beam search decoding for one sentence at a time \n",
        "import numpy as np \n",
        "\n",
        "class Beam():\n",
        "    '''A beam of hypotheses.\n",
        "    Initialized with the an input paragraph and a trained seq2seq model. contains methods for a full beam search decoding\n",
        "    of the model outputs.'''\n",
        "\n",
        "    def __init__(self, model, in_paragraph, beam_size):\n",
        "        \"\"\"\n",
        "        model: the seq2seq model\n",
        "        in_paragraph: a tensor of shape 1 x sequence length representing a single input paragraph (a batch of size 1)\n",
        "        beam_size: int\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = model.device\n",
        "        self.beam_size = beam_size\n",
        "        self.closed_hypotheses = [] #to keep track of outputs which have generated the stop token\n",
        "        self.closed_hypotheses_probs = torch.tensor([]).to(self.device)\n",
        "\n",
        "        decoder_init_states = model.initStates(in_paragraph)  # a tuple (decoder_init_h, decoder_init_c), each of shape (num_layers x 1 x lstm_hidden_size)\n",
        "\n",
        "        # creates a 'batch' of size beam_size, all initialized with the same decoder states\n",
        "        decoder_init_h, decoder_init_c = decoder_init_states  # each should be of shape (num_layers x 1 x lstm_hidden_size) (batch_size = 1)\n",
        "        decoder_init_h = decoder_init_h.repeat(1, beam_size, 1)   # shape (num_layers x beam_size x lstm_hidden_size)\n",
        "        decoder_init_c = decoder_init_c.repeat(1, beam_size, 1)   # shape (num_layers x beam_size x lstm_hidden_size)\n",
        "        self.decoder_init_states = decoder_init_h, decoder_init_c # we will need it at each decoding step (represents encoder output)\n",
        "        self._full_decoder_init_states = decoder_init_h, decoder_init_c #save this for resizing when we need to change batch size \n",
        "        \n",
        "        # only the last layer is used to compute the distribution for the first token\n",
        "        init_h_last_layer = decoder_init_h[-1,0,:]  # shape (lstm_hidden_size)\n",
        "\n",
        "        distributions = self.model.distribution_nn(init_h_last_layer) # shape (vocab_size)\n",
        "        distributions = torch.nn.functional.softmax(distributions, dim = 0) # shape (vocab_size)\n",
        "\n",
        "        # chooses the first tokens\n",
        "        # we sample from the probability distribution to provide more varried hypotheses\n",
        "        chosen_indices = torch.multinomial(input=distributions, num_samples=beam_size, replacement=False)  # shape beam_size\n",
        "        chosen_scores = distributions[chosen_indices]  # shape beam_size\n",
        "\n",
        "        # the probability of each hypothesis in the beam\n",
        "        # convert to log probabilities so we can sum instead of multiplying probabilities\n",
        "        self.open_hypotheses_probs = torch.log(chosen_scores).view(-1, 1) #size beam_size x 1 to start, will be smaller as hypotheses are closed\n",
        "\n",
        "        # contains the first token of each hypothesis \n",
        "        self.open_hypotheses = chosen_indices.view(self.beam_size, 1) #size beam size x 1\n",
        "\n",
        "    def __str__(self): \n",
        "        open_strings = ''\n",
        "        if(len(self.open_hypotheses.squeeze())):\n",
        "            open_strings = [f\"{self.open_hypotheses_probs[i].item()}\\t{ids_to_texts([self.open_hypotheses[i]], decoding_map, END_id, word_tokens)[0]}\" for i in range(len(self.open_hypotheses))]\n",
        "        closed_strings = [f\"{self.closed_hypotheses_probs[i].item()}\\t{ids_to_texts([self.closed_hypotheses[i]], decoding_map, END_id, word_tokens)[0]}\" for i in range(len(self.closed_hypotheses))]\n",
        "        return 'Probs\\tOpen hypotheses\\n' + \"\\n\".join(open_strings) + '\\nProbs\\tclosed hypotheses\\n' + \"\\n\".join(closed_strings)+'\\n'\n",
        "\n",
        "    def _resize_init_states(self): \n",
        "        #helper function to make sure the init states for the model match the shape of the open hypotheses in the beam \n",
        "        decoder_init_h, decoder_init_c = self._full_decoder_init_states  # init states from last decoding step, size num_layers x open_size x lstm_hidden_size \n",
        "        decoder_init_h = decoder_init_h[:, :len(self.open_hypotheses), :].contiguous()  # shape (num_layers x open_size x lstm_hidden_size)\n",
        "        decoder_init_c = decoder_init_c[:, :len(self.open_hypotheses), :].contiguous()  # shape (num_layers x open_size x lstm_hidden_size)\n",
        "        self.decoder_init_states = decoder_init_h, decoder_init_c #\n",
        "\n",
        "    def _decode_step(self, verbose = True):\n",
        "        \"\"\"\n",
        "        computes probabilites for continuation character for each hypothesis in the beam, then select the \n",
        "        K most likely sequences, and update hypotheses and probabilities \n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            char_embeds                 = self.model.char_embeddings(self.open_hypotheses) # num_open_hyp x seq_length (current decoding step) x embedding_size \n",
        "            self._resize_init_states()\n",
        "            decoder_output, _           = self.model.decoder_lstm(char_embeds, self.decoder_init_states) # open_hyps x seq_length x hidden size \n",
        "            decoder_output              = decoder_output[:,-1, :] # open_hyps x hidden_size (we keep only the last token representation)\n",
        "            char_distributions          = self.model.distribution_nn(decoder_output) # open_hyps x vocab_size\n",
        "\n",
        "            # adds probabilities of the hypotheses so far to probabilities for the next token\n",
        "            char_distributions          = torch.nn.functional.log_softmax(char_distributions, dim = 1) #open_hyps x vocab size \n",
        "            scores                      = char_distributions + self.open_hypotheses_probs #open_hyps x vocab size \n",
        "            # turn log probs back into probs for multinomial \n",
        "            scores                      = torch.exp(scores).flatten() # shape open_hypothese * vocab_size\n",
        "            #concatinate with the probabilities for closed hypotheses in the beam\n",
        "            scores                      = torch.cat([scores, torch.exp(self.closed_hypotheses_probs.flatten())]) #beam_size*vocab_size\n",
        "\n",
        "            # sample k scores based off their probabilities\n",
        "            chosen_indices              = torch.multinomial(input=scores, num_samples=self.beam_size, replacement=False)  # shape beam_size\n",
        "\n",
        "            #seperate scores and indices for open vs closed hypotheses \n",
        "            open_chosen_indices         = chosen_indices[(chosen_indices<len(self.open_hypotheses)*self.model.size_vocabulary)] #num_open \n",
        "            closed_chosen_indices       = chosen_indices[(chosen_indices>=len(self.open_hypotheses)*self.model.size_vocabulary)] #num_closed\n",
        "            closed_chosen_indices       -= len(self.open_hypotheses)*self.model.size_vocabulary #num_closed\n",
        "\n",
        "            #get closed hypothese and probs which are kept\n",
        "            self.closed_hypotheses        = np.take(self.closed_hypotheses, closed_chosen_indices.tolist(), axis=0).tolist()\n",
        "            self.closed_hypotheses_probs  = self.closed_hypotheses_probs[closed_chosen_indices]\n",
        "\n",
        "            #for open hypothese and scores, we compute the next char and add probabilities \n",
        "            # tensor of shape beam_size containing the id of every selected token to continuate the hypotheses\n",
        "            open_token_continuations      = open_chosen_indices % self.model.size_vocabulary #shape num_open_hypotheses\n",
        "            # tensor of shape beam_size containing the id of every hypothesis selected to be continuated\n",
        "            open_hypotheses_continuations = torch.div(open_chosen_indices, self.model.size_vocabulary).int() #shape num_open_hypotheses\n",
        "\n",
        "            # updates the openhypotheses' scores\n",
        "            open_hypotheses_probs         = torch.log(scores[open_chosen_indices]).view(-1, 1)  # change back to logs so we can sum instead of multiplying\n",
        "\n",
        "            # updates the hypotheses with newly generated chars \n",
        "            continued_hypotheses          = self.open_hypotheses[open_hypotheses_continuations.long()] \n",
        "            continued_hypotheses          = torch.cat([continued_hypotheses, open_token_continuations.view(-1, 1)], dim = 1)  # adds the selected token to each hyp\n",
        "            \n",
        "            #mask for newly closed elements anad split into open and newly closed hypotheses\n",
        "            newly_closed_mask             = (open_token_continuations == self.model.EOP_id).view(-1) #size open_hypotheses\n",
        "            self.open_hypotheses          = continued_hypotheses[(newly_closed_mask==False)]  #size new open_hypotheses\n",
        "            self.open_hypotheses_probs    = open_hypotheses_probs[(newly_closed_mask==False)]#size new open_hypotheses\n",
        "\n",
        "\n",
        "            newly_closed_hypotheses = continued_hypotheses[newly_closed_mask] #number of newly closed hypotheses\n",
        "            newly_closed_probs = open_hypotheses_probs[newly_closed_mask].view(-1) \n",
        "            \n",
        "            self.closed_hypotheses.extend(newly_closed_hypotheses.tolist())\n",
        "            self.closed_hypotheses_probs = torch.cat([self.closed_hypotheses_probs, newly_closed_probs])\n",
        "\n",
        "        if verbose: print(str(self))\n",
        "\n",
        "\n",
        "    def decode(self, max_paragraph_length, verbose = True): \n",
        "        '''\n",
        "        Decodes a paragraph, outputs the best hypothesis found using beam search.\n",
        "        max_paragraph_length: int\n",
        "        '''\n",
        "        for i in range(max_paragraph_length):\n",
        "            self._decode_step(verbose)\n",
        "\n",
        "            # If all hypotheses contain EOP, stop\n",
        "            if len(self.open_hypotheses)==0: \n",
        "              break\n",
        "        #if no sequences were closed, output the best open sequence \n",
        "        if len(self.closed_hypotheses) == 0 : #there are no closed hypotheses\n",
        "          return self.open_hypotheses[torch.argmax(self.open_hypotheses_probs)]\n",
        "        #otherwise, output the best closed hypothesis\n",
        "        return self.closed_hypotheses[torch.argmax(self.closed_hypotheses_probs)]     "
      ],
      "metadata": {
        "id": "1hW5DnApwnpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_size = 5\n",
        "\n",
        "#get an example sentence\n",
        "batch = batch_generator.get_batch(1)\n",
        "print(ids_to_texts(batch[0], decoding_map, END_id, word_tokens=word_tokens))\n",
        "\n",
        "beam = Beam(model, batch[0].to(model.device), beam_size)\n",
        "out_paragraph = beam.decode(50, verbose = True)\n",
        "len(out_paragraph)\n",
        "\n",
        "print(ids_to_texts([out_paragraph], decoding_map, END_id, word_tokens=word_tokens))"
      ],
      "metadata": {
        "id": "iCBioufiXxYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(beam))"
      ],
      "metadata": {
        "id": "uZHaHutslTVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2GHjtPr4Mw2"
      },
      "source": [
        "Read the remarks at the beginning of the TP again.\n",
        "\n",
        "Once you are sure that your system is correctly implemented and generates texts that look a little bit like natural language, find ways to improve the system.\n",
        "Here are some ideas (ordered arbitrarily):\n",
        "\n",
        "*   Compute a measure that evaluates the performance of the model.\n",
        "*   Split your dataset into a training and a development section, and use this split in a relevant way.\n",
        "*   Implement beam decoding instead of greedy decoding.\n",
        "*   Use other units of text instead of characters (ex: words, word-pieces).\n",
        "*   Add more data to the dataset.\n",
        "*   Use graphs to visualise the training process and the predictions.\n",
        "\n",
        "Document in a text cell all of the changes that you make to the system and describe their impact."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of our improvements\n",
        "\n",
        "**evaluating models:**\n",
        "To evaluate models, we use both cross-entropy loss and a perplexity score. To calculate these, first we split our data into training, validation, and test splits. During training, cross-entropy loss is calculated on the training and validation splits (and used for early stopping), while perplexity is calculated on the validation split. We define perplexity as the sum of the inverse log probabilities for each sentence in the validation split. As shown in the graphs after training, these two measures roughly correlate with eachother. \n",
        "\n",
        "**tokenization strategies:**\n",
        "we implement two dokenization strategies: a character-by-character one and a word-level one using polyglot as the tokenizer. this can be controlled by a variable \"TOKENIZATION\" in the first code window. Word-level tokenization has the advantage of ensuring that outputs will at least be real words, but struggles with unknown words in the dev and test sets, and also can only output words seen in training. character-level tokenization on the other had produces outputs full of spelling errors, but does not have the same vocabulary issue. \n",
        "\n",
        "\n",
        "\n",
        "**beam search:**\n",
        "Observationally, adding beam search decoding makes the model's outputs resemble natural language much more closely. with raw outputs, there are frequent misspellings even for well-trained models, however with beam search spelling is overall much better. Additionally, punctuation patterns are much more consistent and gramatical. \n",
        "\n",
        "**limitations:** in our implementation of beam search, there is no normalization for hypothesis lengths. this means that as hypotheses get longer, the probabilities naturally get lower. this has two unwanted effects: \n",
        "\n",
        "\n",
        "1.   Shorter hypotheses are preferred. this means that the first hypothesis to generate the stop token will almost always be the one selected. \n",
        "2.   for poorly trained models, they may arrive at a situation where the the probabilites for all continuations are too low to select, resulting in a crash. this is more likely for models which have not been trained enough, and is also more likley when using character tokenization (as character sequences are longer than word sequences). If this happens, consider retraining the model or decreasing the maximum output length. \n",
        "\n",
        "Additionally, the model has a limited sensitivity to the input paragraph: it can capture some information about the input paragraph but without any detail. \n",
        "\n"
      ],
      "metadata": {
        "id": "Zyobjqg1v_UI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-18GCLhewDE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
